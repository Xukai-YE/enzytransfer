#!/usr/bin/env python3
"""
åºåˆ—å¯¹æ¯”ä¸åˆå¹¶å·¥å…· - å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿæ–‡ä»¶å¤„ç†
"""

import yaml
import os
import sys
import argparse
from pathlib import Path
from collections import defaultdict
import difflib
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import hashlib

# å°è¯•ä½¿ç”¨æ›´å¿«çš„CLoader
try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper
    print("âš ï¸  æç¤º: å®‰è£…libyamlå¯ä»¥åŠ é€ŸYAMLå¤„ç† (pip install pyyaml --global-option='--with-libyaml')")

def compute_sequence_hash(sequence):
    """è®¡ç®—åºåˆ—çš„hashå€¼ï¼Œç”¨äºå¿«é€Ÿæ¯”è¾ƒ"""
    if not sequence:
        return None
    return hashlib.md5(sequence.encode()).hexdigest()

def extract_info_from_yaml(yaml_path):
    """ä»YAMLæ–‡ä»¶ä¸­æå–å…³é”®ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            # ä½¿ç”¨CLoaderåŠ é€Ÿ
            data = yaml.load(f, Loader=Loader)
        
        proteins = data.get('proteins', [])
        if not proteins:
            return None
        
        protein = proteins[0]
        
        wildtype_seq = protein.get('wildtype_sequence')
        mutant_seq = protein.get('mutant_sequence')
        
        info = {
            'file_path': str(yaml_path),
            'file_name': yaml_path.name,
            'uniprotid': protein.get('uniprotid'),
            'variant_type': protein.get('variant_type'),
            'variant_description': protein.get('variant_description'),
            'wildtype_sequence': wildtype_seq,
            'mutant_sequence': mutant_seq,
            'wildtype_hash': compute_sequence_hash(wildtype_seq),  # ç”¨äºå¿«é€Ÿæ¯”è¾ƒ
            'mutant_hash': compute_sequence_hash(mutant_seq),
            'organism': protein.get('organism'),
            'ecnumber': protein.get('ecnumber'),
            'full_data': data
        }
        
        return info
    
    except Exception as e:
        print(f"âš ï¸  è¯»å–æ–‡ä»¶å‡ºé”™ {yaml_path}: {e}")
        return None

def scan_yaml_files(directories):
    """æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰YAMLæ–‡ä»¶"""
    yaml_files = []
    
    for directory in directories:
        dir_path = Path(directory)
        if not dir_path.exists():
            print(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {directory}")
            continue
        
        for yaml_file in dir_path.rglob('*.yaml'):
            yaml_files.append(yaml_file)
        for yaml_file in dir_path.rglob('*.yml'):
            yaml_files.append(yaml_file)
    
    return yaml_files

def process_files_parallel(yaml_files, max_workers=None):
    """
    å¹¶è¡Œå¤„ç†YAMLæ–‡ä»¶
    max_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
    """
    if max_workers is None:
        max_workers = min(cpu_count(), len(yaml_files))
    
    yaml_infos = []
    
    print(f"ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†...")
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(extract_info_from_yaml, yaml_file): yaml_file 
            for yaml_file in yaml_files
        }
        
        # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
        completed = 0
        total = len(yaml_files)
        
        for future in as_completed(future_to_file):
            completed += 1
            if completed % 10 == 0 or completed == total:
                print(f"  å¤„ç†è¿›åº¦: {completed}/{total} ({completed*100//total}%)", end='\r')
            
            try:
                info = future.result()
                if info:
                    yaml_infos.append(info)
            except Exception as e:
                yaml_file = future_to_file[future]
                print(f"\nâš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ {yaml_file}: {e}")
        
        print()  # æ¢è¡Œ
    
    return yaml_infos

def group_by_uniprot_mutation(yaml_infos):
    """æŒ‰ç…§uniprotidå’Œmutationåˆ†ç»„"""
    groups = defaultdict(list)
    
    for info in yaml_infos:
        if info and info['uniprotid']:
            mutation = info['variant_description'] if info['variant_description'] else 'wildtype'
            key = (info['uniprotid'], mutation)
            groups[key].append(info)
    
    return groups

def compare_sequences_fast(info_list):
    """
    å¿«é€Ÿåºåˆ—æ¯”è¾ƒï¼ˆä½¿ç”¨hashï¼‰
    è¿”å›: (æ˜¯å¦ä¸€è‡´, æ¶ˆæ¯)
    """
    if len(info_list) < 2:
        return True, "Only one file"
    
    # å…ˆç”¨hashå¿«é€Ÿæ¯”è¾ƒ
    wildtype_hashes = [f['wildtype_hash'] for f in info_list if f['wildtype_hash']]
    if len(wildtype_hashes) > 1:
        if len(set(wildtype_hashes)) > 1:
            return False, "Wildtype sequences differ"
    
    mutant_hashes = [f['mutant_hash'] for f in info_list if f['mutant_hash']]
    if len(mutant_hashes) > 1:
        if len(set(mutant_hashes)) > 1:
            return False, "Mutant sequences differ"
    
    return True, "All sequences are identical"

def merge_yaml_files(file_infos, output_dir):
    """åˆå¹¶å¤šä¸ªYAMLæ–‡ä»¶ä¸ºä¸€ä¸ª"""
    
    if not file_infos:
        return None
    
    base_data = file_infos[0]['full_data'].copy()
    base_info = file_infos[0]
    
    merged_data = {
        'version': '2.0',
        'created': datetime.now().isoformat(),
        'modified': None,
        'creators': [],
        'vessels': None,
        'proteins': base_data.get('proteins', [])[:1],
        'complexes': None,
        'small_molecules': [],
        'reactions': [],
        'measurements': [],
        'equations': None,
        'parameters': [],
        'buffer': {'buffer': None, 'buffer_concentration': None},
        'references': []
    }
    
    # åˆå¹¶creators
    creators_set = set()
    for info in file_infos:
        data = info['full_data']
        for creator in data.get('creators', []):
            creator_tuple = (
                creator.get('given_name', ''),
                creator.get('family_name', ''),
                creator.get('mail', '')
            )
            creators_set.add(creator_tuple)
    
    merged_data['creators'] = [
        {'given_name': c[0], 'family_name': c[1], 'mail': c[2]}
        for c in sorted(creators_set)
    ]
    
    # åˆå¹¶small_molecules
    molecules_dict = {}
    for info in file_infos:
        data = info['full_data']
        for mol in data.get('small_molecules', []):
            mol_id = mol.get('id')
            if mol_id and mol_id not in molecules_dict:
                molecules_dict[mol_id] = mol
            elif mol_id and mol.get('name'):
                if not molecules_dict[mol_id].get('name'):
                    molecules_dict[mol_id] = mol
    
    merged_data['small_molecules'] = list(molecules_dict.values())
    
    # åˆå¹¶reactions, measurements, parameters
    for info in file_infos:
        data = info['full_data']
        merged_data['reactions'].extend(data.get('reactions', []))
        merged_data['measurements'].extend(data.get('measurements', []))
        merged_data['parameters'].extend(data.get('parameters', []))
    
    # åˆå¹¶references
    references_dict = {}
    for info in file_infos:
        data = info['full_data']
        for ref in data.get('references', []):
            key = ref.get('pubmed_id') or ref.get('title') or str(len(references_dict))
            if key not in references_dict:
                references_dict[key] = ref
    
    merged_data['references'] = list(references_dict.values())
    
    # åˆå¹¶buffer
    buffer_info = []
    for info in file_infos:
        data = info['full_data']
        buffer_data = data.get('buffer', {})
        if buffer_data and buffer_data.get('buffer'):
            buffer_info.append(buffer_data.get('buffer'))
    
    if buffer_info:
        merged_data['buffer']['buffer'] = '; '.join(set(buffer_info))
    
    # ç”Ÿæˆæ–‡ä»¶å
    uniprotid = base_info['uniprotid']
    mutation = base_info['variant_description'] or 'wildtype'
    safe_organism = (base_info['organism'] or 'Unknown').replace(' ', '_').replace('/', '_')
    safe_mutation = mutation.replace(' ', '_').replace('/', '_')
    
    merged_filename = f"MERGED_{uniprotid}_{safe_mutation}.yaml"
    
    merged_data['name'] = f"Merged entry for {uniprotid} {mutation}"
    merged_data['description'] = f"Merged from {len(file_infos)} files: " + ", ".join([f['file_name'] for f in file_infos])
    
    # å†™å…¥æ–‡ä»¶ï¼ˆä½¿ç”¨CDumperåŠ é€Ÿï¼‰
    output_path = os.path.join(output_dir, merged_filename)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(merged_data, f, Dumper=Dumper, default_flow_style=False, 
                  allow_unicode=True, sort_keys=False)
    
    return output_path

def main():
    parser = argparse.ArgumentParser(
        description='åºåˆ—å¯¹æ¯”ä¸åˆå¹¶å·¥å…· - å¹¶è¡Œä¼˜åŒ–ç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python merge_sequences_fast.py /data/sabio /data/brenda -o merged
  python merge_sequences_fast.py /data/db1 /data/db2 -j 8  # ä½¿ç”¨8ä¸ªè¿›ç¨‹
        """
    )
    
    parser.add_argument('directories', nargs='+', 
                       help='è¦æ‰«æçš„ç›®å½•è·¯å¾„(è‡³å°‘éœ€è¦2ä¸ªç›®å½•)')
    parser.add_argument('-o', '--output-dir', default='merged_output', 
                       help='åˆå¹¶åçš„YAMLæ–‡ä»¶è¾“å‡ºç›®å½• (é»˜è®¤: merged_output)')
    parser.add_argument('-j', '--jobs', type=int, default=None,
                       help='å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°)')
    parser.add_argument('--allow-single-dir', action='store_true',
                       help='å…è®¸åªæ‰«æå•ä¸ªç›®å½•ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('-v', '--verbose', action='store_true', 
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("åºåˆ—å¯¹æ¯”ä¸åˆå¹¶å·¥å…· - å¹¶è¡Œä¼˜åŒ–ç‰ˆ ğŸš€")
    print("=" * 80)
    print()
    
    # æ£€æŸ¥ç›®å½•æ•°é‡
    if len(args.directories) < 2 and not args.allow_single_dir:
        print("âŒ é”™è¯¯: åˆå¹¶å·¥å…·éœ€è¦è‡³å°‘2ä¸ªç›®å½•ä½œä¸ºè¾“å…¥!")
        print()
        print("åˆå¹¶å·¥å…·çš„ä½¿ç”¨åœºæ™¯æ˜¯æ•´åˆæ¥è‡ªä¸åŒæ•°æ®åº“çš„æ•°æ®ï¼Œä¾‹å¦‚:")
        print("  python3 merge_sequences_fast.py /data/sabio_output /data/brenda_output")
        print()
        print("å¦‚æœä½ åªæƒ³æ£€æŸ¥å•ä¸ªç›®å½•ï¼Œè¯·ä½¿ç”¨å¯¹æ¯”å·¥å…·:")
        print("  python3 compare_sequences_cli.py /data/your_folder")
        print()
        print("å¦‚æœç¡®å®éœ€è¦åœ¨å•ä¸ªç›®å½•ä¸­æŸ¥æ‰¾å¹¶åˆå¹¶é‡å¤é¡¹ï¼Œè¯·æ·»åŠ  --allow-single-dir é€‰é¡¹")
        sys.exit(1)
    
    if len(args.directories) == 1 and args.allow_single_dir:
        print("âš ï¸  è­¦å‘Š: åªæ‰«æå•ä¸ªç›®å½•ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        print()
    
    # éªŒè¯ç›®å½•
    valid_dirs = []
    for directory in args.directories:
        if os.path.exists(directory):
            valid_dirs.append(directory)
            print(f"âœ“ æ‰¾åˆ°ç›®å½•: {directory}")
        else:
            print(f"âœ— ç›®å½•ä¸å­˜åœ¨: {directory}")
    
    if not valid_dirs:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„ç›®å½•!")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    print(f"\nâœ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # ç¡®å®šè¿›ç¨‹æ•°
    max_workers = args.jobs or cpu_count()
    print(f"âœ“ ä½¿ç”¨ {max_workers} ä¸ªCPUæ ¸å¿ƒ")
    
    print(f"\n{'='*80}")
    print("é˜¶æ®µ 1/4: æ‰«æYAMLæ–‡ä»¶")
    print('='*80)
    
    # æ‰«ææ–‡ä»¶
    yaml_files = scan_yaml_files(valid_dirs)
    print(f"æ‰¾åˆ° {len(yaml_files)} ä¸ªYAMLæ–‡ä»¶")
    
    if not yaml_files:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°YAMLæ–‡ä»¶!")
        sys.exit(1)
    
    print(f"\n{'='*80}")
    print("é˜¶æ®µ 2/4: å¹¶è¡Œæå–æ–‡ä»¶ä¿¡æ¯")
    print('='*80)
    
    # å¹¶è¡Œå¤„ç†æ–‡ä»¶
    yaml_infos = process_files_parallel(yaml_files, max_workers=max_workers)
    
    print(f"âœ“ æˆåŠŸæå– {len(yaml_infos)} ä¸ªæ–‡ä»¶çš„ä¿¡æ¯")
    
    if args.verbose:
        print("\næå–çš„æ–‡ä»¶åˆ—è¡¨:")
        for info in yaml_infos[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {info['uniprotid']} | {info['variant_description']} | {info['file_name']}")
        if len(yaml_infos) > 10:
            print(f"  ... è¿˜æœ‰ {len(yaml_infos) - 10} ä¸ªæ–‡ä»¶")
    
    print(f"\n{'='*80}")
    print("é˜¶æ®µ 3/4: åˆ†ç»„å’Œåºåˆ—æ£€æŸ¥")
    print('='*80)
    
    # æŒ‰uniprotidå’Œmutationåˆ†ç»„
    groups = group_by_uniprot_mutation(yaml_infos)
    print(f"æ‰¾åˆ° {len(groups)} ä¸ªä¸åŒçš„ uniprotid+mutation ç»„åˆ")
    
    # æ‰¾å‡ºæœ‰å¤šä¸ªæ–‡ä»¶çš„ç»„
    duplicate_groups = {k: v for k, v in groups.items() if len(v) > 1}
    
    print(f"å‘ç° {len(duplicate_groups)} ä¸ªæœ‰é‡å¤çš„ç»„åˆ")
    
    if not duplicate_groups:
        print("\næ²¡æœ‰å‘ç°é‡å¤çš„uniprotid+mutationç»„åˆ,æ— éœ€åˆå¹¶ã€‚")
        sys.exit(0)
    
    print(f"\n{'='*80}")
    print("é˜¶æ®µ 4/4: æ‰§è¡Œåˆå¹¶")
    print('='*80)
    
    # å¯¹æ¯ä¸ªé‡å¤ç»„è¿›è¡Œå¤„ç†
    merged_count = 0
    skipped_count = 0
    
    for idx, ((uniprotid, mutation), files) in enumerate(duplicate_groups.items(), 1):
        print(f"\n[{idx}/{len(duplicate_groups)}] {uniprotid} | {mutation} ({len(files)}ä¸ªæ–‡ä»¶)")
        
        if args.verbose:
            for f in files:
                print(f"  - {f['file_name']}")
        
        # å¿«é€Ÿåºåˆ—æ£€æŸ¥ï¼ˆä½¿ç”¨hashï¼‰
        is_identical, msg = compare_sequences_fast(files)
        
        if is_identical:
            print(f"  âœ“ åºåˆ—ä¸€è‡´ï¼Œæ­£åœ¨åˆå¹¶...")
            
            try:
                merged_file = merge_yaml_files(files, output_dir)
                if merged_file:
                    print(f"  âœ“ åˆå¹¶æˆåŠŸ: {os.path.basename(merged_file)}")
                    merged_count += 1
                else:
                    print("  âœ— åˆå¹¶å¤±è´¥")
                    skipped_count += 1
            except Exception as e:
                print(f"  âœ— åˆå¹¶å¤±è´¥: {e}")
                skipped_count += 1
        else:
            print(f"  âš ï¸  {msg}ï¼Œè·³è¿‡åˆå¹¶")
            skipped_count += 1
    
    # æ€»ç»“
    print(f"\n{'=' * 80}")
    print("âœ… å¤„ç†å®Œæˆ!")
    print("=" * 80)
    print(f"æ€»å…±æ‰«æ: {len(yaml_files)} ä¸ªYAMLæ–‡ä»¶")
    print(f"æˆåŠŸè§£æ: {len(yaml_infos)} ä¸ªæ–‡ä»¶")
    print(f"å‘ç°é‡å¤ç»„åˆ: {len(duplicate_groups)} ä¸ª")
    print(f"æˆåŠŸåˆå¹¶: {merged_count} ä¸ª")
    print(f"è·³è¿‡: {skipped_count} ä¸ª")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 80)

if __name__ == "__main__":
    main()
